激活函数
如果输入变化很小，导致输出结构发生截然不同的结果，这种情况是我们不希望看到的，为了模拟更细微的变化，输入和输出数值不只是0到1，可以是0和1之间的任何数，

激活函数是用来加入非线性因素的，因为线性模型的表达力不够
这句话字面的意思很容易理解，但是在具体处理图像的时候是什么情况呢？我们知道在神经网络中，对于图像，我们主要采用了卷积的方式来处理，也就是对每个像素点赋予一个权值，这个操作显然就是线性的。但是对于我们样本来说，不一定是线性可分的，为了解决这个问题，我们可以进行线性变化，或者我们引入非线性因素，解决线性模型所不能解决的问题。
这里插一句，来比较一下上面的那些激活函数，因为神经网络的数学基础是处处可微的，所以选取的激活函数要能保证数据输入与输出也是可微的，运算特征是不断进行循环计算，所以在每代循环过程中，每个神经元的值也是在不断变化的。
这就导致了tanh特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果显示出来，但有是，在特征相差比较复杂或是相差不是特别大时，需要更细微的分类判断的时候，sigmoid效果就好了。
还有一个东西要注意，sigmoid 和 tanh作为激活函数的话，一定要注意一定要对 input 进行归一话，否则激活后的值都会进入平坦区，使隐层的输出全部趋同，但是 ReLU 并不需要输入归一化来防止它们达到饱和。

构建稀疏矩阵，也就是稀疏性，这个特性可以去除数据中的冗余，最大可能保留数据的特征，也就是大多数为0的稀疏矩阵来表示。其实这个特性主要是对于Relu，它就是取的max(0,x)，因为神经网络是不断反复计算，实际上变成了它在尝试不断试探如何用一个大多数为0的矩阵来尝试表达数据特征，结果因为稀疏特性的存在，反而这种方法变得运算得又快效果又好了。所以我们可以看到目前大部分的卷积神经网络中，基本上都是采用了ReLU 函数。

常用的激活函数
激活函数应该具有的性质：
（1）非线性。线性激活层对于深层神经网络没有作用，因为其作用以后仍然是输入的各种线性变换。。
（2）连续可微。梯度下降法的要求。
（3）范围最好不饱和，当有饱和的区间段时，若系统优化进入到该段，梯度近似为0，网络的学习就会停止。
（4）单调性，当激活函数是单调时，单层神经网络的误差函数是凸的，好优化。
（5）在原点处近似线性，这样当权值初始化为接近0的随机值时，网络可以学习的较快，不用可以调节网络的初始值。
目前常用的激活函数都只拥有上述性质的部分，没有一个拥有全部的～～
------------------------------------------------
版权声明：本文为CSDN博主「yjl9122」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/yjl9122/article/details/70198357