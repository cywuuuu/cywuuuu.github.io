## Resnet

## 退化问题

## 梯度消散/爆炸

**原因**
梯度消散和梯度爆炸本质上是一样的，都是因为网络层数太深而引发的梯度反向传播中的连乘效应。Sigmoid激活函数最容易产生梯度消散，这是由于它的函数特性决定的。
Sigmoid函数图像如下图：

![img](https://img2020.cnblogs.com/blog/2097416/202007/2097416-20200728110628825-218399944.jpg)

其导数图像如下图：

![img](https://img2020.cnblogs.com/blog/2097416/202007/2097416-20200728110628342-619056957.jpg)

由图中可以知道，如果使用Sigmoid作为激活函数，那么其梯度不可能超过0.25，当层数叠加，经过链式求导后。很容易产生梯度消失。

**应对方法**

- 改换激活函数，使用relu、LeakyRelu、ELU等激活函数可以改善梯度消散或爆炸问题。relu导数的正数部分恒等于1，所以不会产生梯度消失和梯度爆炸。
- BatchNormalization。对每一层的输入做scale和shift方法，将每层神经元的输入分布强行拉回均值为0、方差为1的标准正态分布，这就使得激活层输入值落入在非线性函数对输入值比较敏感的区域，使得输入的小变化会导致损失函数较大的变化，使得梯度变大，训练速度加快，且避免梯度消失问题。
- ResNet残差结构。
- 梯度剪切，该方法主要是针对梯度爆炸提出。其思想是设置一个梯度剪切阈值，更新梯度时，如果梯度超过这个阈值，那么限制其在这个范围之内。

# RESNET

自2012年Alex Krizhevsky利用深度卷积神经网络（CNN）（AlexNet ）取得ImageNet比赛冠军起，CNN在计算机视觉方面的应用引起了大家广泛地讨论与研究，也涌现了一大批优秀的CNN模型。研究人员发现，网络的深度对CNN的效果影响非常大，**但是单纯地增加网络深度并不能简单地提高网络的效果，由于梯度发散，反而可能损害模型的效果。而shortcut的引入就是解决这个问题的妙招**。本文主要就模型发展中的shortcut展开讨论。

残差网络通过加入 shortcut connections，变得更加容易被优化。包含一个 shortcut connection 的几层网络被称为一个残差块（residual block）

 

![img](https://www.freesion.com/images/906/f17adc805184c9132d90124ccec766a2.png)

​                                                                                 图  残差块